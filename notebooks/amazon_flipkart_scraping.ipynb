{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "import matplotlib.dates as mdates\n",
    "import matplotlib.ticker as ticker\n",
    "from urllib.request import urlopen\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------WEB SCRAPING FROM FLIPKART (only function definition is here)-------------------------------------\n",
    "# ----------------------------------------(Don't run unless urgent)---------------------------------------------------------\n",
    "no_pages = 2\n",
    "\n",
    "def get_data(pageNo):  \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    r = requests.get('https://www.flipkart.com/search?q=tshirts+for+men&otracker=search&otracker1=search&marketplace=FLIPKART&as-show=on&as=off&page='+str(pageNo), headers=headers)#, proxies=proxies)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "#     print(soup)\n",
    "\n",
    "    alls = []\n",
    "    c=0\n",
    "    for d in soup.findAll('div', attrs={'style':'width:25%'}):\n",
    "#         ----------------------#to test code, restricted iterations-----------------------\n",
    "#         c+=1\n",
    "#         if c==10:\n",
    "#             break\n",
    "#         time.sleep(random.random()/100)\n",
    "    \n",
    "# -------------FINDING ALL DATA BY HTML PARSING AND CLASS IS FOUND MANUALLY FROM HTML CODE BY PRINTING SOUP----------------\n",
    "        brand=d.find('div',attrs={'class':'_2B_pmu'})\n",
    "#         print(brand)\n",
    "        name=d.find('a',attrs={'class':'_2mylT6'})\n",
    "        if name is \"\":\n",
    "            name=d.find('a',attrs={'class':'_2mylT6 _3Ockxk'})\n",
    "        price = d.find('div', attrs={'class':'_1vC4OE'})\n",
    "#         print(price)\n",
    "\n",
    "# ------------------------CREATING COLUMNS OF BRAND, NAME, PRICE, ETC BY GETTING TEXT FROM HTML TAGS------------------------\n",
    "        all1=[]\n",
    "        if brand is not None:\n",
    "            all1.append(brand.text)\n",
    "        else:\n",
    "            all1.append(\"\")\n",
    "        \n",
    "        if name is not None:\n",
    "            all1.append(name.text)\n",
    "        else:\n",
    "            all1.append(\"\")\n",
    "    \n",
    "        url = d.find('div', attrs={'class':'IIdQZO _1SSAGr'})\n",
    "        if url==None:\n",
    "#             all1.append(\"\")\n",
    "            continue\n",
    "        n = url.find_all('a', attrs={'class':'_3dqZjq'},href=True)\n",
    "        \n",
    "# ------------------------------------VISITING URL OF EVERY PRODUCT AND GETTING FEATURES------------------------------------       \n",
    "            \n",
    "        r = requests.get('https://www.flipkart.com'+str(n[0]['href']), headers=headers)#, proxies=proxies)\n",
    "        content2 = r.content\n",
    "        soup2 = BeautifulSoup(content2)\n",
    "        rt=''\n",
    "        for rating in soup2.findAll('div',attrs={'class':'hGSR34 bqXGTW'}):\n",
    "            rt=float(rating.text)\n",
    "#             if rating is not None:\n",
    "#                 all1.append(rating.text)\n",
    "#             else:\n",
    "#                 all1.append('')\n",
    "        all1.append(rt)\n",
    "        if price is not None:\n",
    "            all1.append(float(price.text.replace('â‚¹','').replace(',','')))\n",
    "        else:\n",
    "            all1.append('')\n",
    "        \n",
    "        all1.append('https://www.flipkart.com'+str(n[0]['href']))\n",
    "        \n",
    "        ratings=''\n",
    "        for rt in soup2.findAll('span',attrs={'class':'_38sUEc _1je6zX'}):\n",
    "            ratings=int(rt.text.split()[0].replace(',',''))\n",
    "        all1.append(ratings)\n",
    "        for box in soup2.findAll('div',attrs={'class':'_2GNeiG _2t27J6'}):\n",
    "            for d3 in box.findAll('div',attrs={'class':'row'}):\n",
    "                for d4 in d3.findAll('div',attrs={'class':'col col-3-12 _1kyh2f'}):\n",
    "                    if d4.text=='Brand Color':\n",
    "                        color=d3.find('div',attrs={'class':'col col-9-12 _1BMpvA'}).text\n",
    "                    if d4.text=='Sleeve':\n",
    "                        sleeve=d3.find('div',attrs={'class':'col col-9-12 _1BMpvA'}).text\n",
    "                    if d4.text=='Neck Type':\n",
    "                        neck=d3.find('div',attrs={'class':'col col-9-12 _1BMpvA'}).text\n",
    "                    if d4.text=='Fabric':\n",
    "                        fabric=d3.find('div',attrs={'class':'col col-9-12 _1BMpvA'}).text\n",
    "                    if d4.text=='Brand Fit':\n",
    "                        fit=d3.find('div',attrs={'class':'col col-9-12 _1BMpvA'}).text\n",
    "                        \n",
    "        if color is not None:\n",
    "            all1.append(color)\n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        if sleeve is not None:\n",
    "            all1.append(sleeve)    \n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        if neck is not None:\n",
    "            all1.append(neck)\n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        if fabric is not None:\n",
    "            all1.append(fabric)\n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        if fit is not None:\n",
    "            all1.append(fit)\n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        all1.append(pageNo)\n",
    "        alls.append(all1)\n",
    "#         print(len(all1))\n",
    "    return alls\n",
    "get_data(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Brand Name</th>\n",
       "      <th>Shirt Name</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Price</th>\n",
       "      <th>URL</th>\n",
       "      <th>No. of ratings</th>\n",
       "      <th>Colour</th>\n",
       "      <th>Sleeve</th>\n",
       "      <th>Neck</th>\n",
       "      <th>Material</th>\n",
       "      <th>Fit Type</th>\n",
       "      <th>PG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Brand Name, Shirt Name, Rating, Price, URL, No. of ratings, Colour, Sleeve, Neck, Material, Fit Type, PG]\n",
       "Index: []"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------WEB SCRAPING FROM FLIPKART (FUNCTION CALL)--------------------------------------\n",
    "# -----------------------------------------------(Don't run unless urgent)--------------------------------------------------\n",
    "results = []\n",
    "for i in range(1, no_pages+1):\n",
    "    results.append(get_data(i))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "df = pd.DataFrame(flatten(results),columns=['Brand Name','Shirt Name','Rating','Price', 'URL','No. of ratings','Colour','Sleeve','Neck','Material','Fit Type','PG'])\n",
    "df.to_csv('flipkart_products.csv', index=False, encoding='utf-8')\n",
    "# df = pd.read_csv(r\"C:\\Users\\Purva\\Desktop\\Jupyter Notebook\\fktshirtformen.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------WEB SCRAPING FROM AMAZON (only function definition is here)-------------------------------------\n",
    "# ----------------------------------------(Don't run unless urgent)---------------------------------------------------------\n",
    "no_pages = 1\n",
    "\n",
    "def get_data(pageNo):  \n",
    "    headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:66.0) Gecko/20100101 Firefox/66.0\", \"Accept-Encoding\":\"gzip, deflate\", \"Accept\":\"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\", \"DNT\":\"1\",\"Connection\":\"close\", \"Upgrade-Insecure-Requests\":\"1\"}\n",
    "    r = requests.get('https://www.amazon.in/s?k=t-shirt&page='+str(pageNo)+'&qid=1594056503&ref=sr_pg_'+str(pageNo), headers=headers)#, proxies=proxies)\n",
    "    content = r.content\n",
    "    soup = BeautifulSoup(content)\n",
    "    #print(soup)\n",
    "    alls = []\n",
    "    c=0\n",
    "    for d in soup.findAll('div', attrs={'class':'a-section a-spacing-medium a-text-center'}):\n",
    "        c+=1\n",
    "#         if c==20:\n",
    "#             break  #to test code, restricted iterations\n",
    "        \n",
    "        all1=[]\n",
    "        #brand name\n",
    "        brand=d.find('span',attrs={'class':'a-size-base-plus a-color-base','dir':'auto'})    \n",
    "        if brand is not None:\n",
    "            all1.append(brand.text)\n",
    "        else:\n",
    "            all1.append(\"\")\n",
    "\n",
    "        #shirt name    \n",
    "        name=d.find('span',attrs={'class':'a-size-base-plus a-color-base a-text-normal', 'dir':'auto'})\n",
    "        if name is not None:\n",
    "            all1.append(name.text)\n",
    "        else:\n",
    "            all1.append(\"\")\n",
    "        \n",
    "        #rating\n",
    "        rating = d.find('span', attrs={'class':'a-icon-alt'})\n",
    "        if rating is not None:\n",
    "            all1.append(rating.text)\n",
    "        else:\n",
    "            all1.append('')   \n",
    "        \n",
    "        #price\n",
    "        price = d.find('span', attrs={'class':'a-offscreen'})\n",
    "        if price is not None:\n",
    "            all1.append(float(price.text.replace('â‚¹','').replace(',','')))\n",
    "        else:\n",
    "            all1.append('')\n",
    "            \n",
    "        #visiting each product\n",
    "        name = d.find('span', attrs={'class':'rush-component'})\n",
    "        n = name.find_all('a', href=True)\n",
    "        if n==[]:\n",
    "            continue\n",
    "        r = requests.get('https://www.amazon.in'+str(n[0]['href']), headers=headers)#, proxies=proxies)\n",
    "        content2 = r.content\n",
    "        soup2 = BeautifulSoup(content2)\n",
    "        \n",
    "        #link\n",
    "        if n!=[]:\n",
    "            all1.append('https://www.amazon.in'+str(n[0]['href']))  # add site link in table\n",
    "        else:\n",
    "            all1.append('')\n",
    "        \n",
    "        #no. of ratings\n",
    "        ratings=soup2.find('span',attrs={'id':'acrCustomerReviewText','class':'a-size-base'}) \n",
    "        if ratings is not None:\n",
    "            all1.append((ratings.text.replace(',','')))\n",
    "        else:\n",
    "            all1.append(0)\n",
    "        \n",
    "        \n",
    "        #features and colour        \n",
    "        s=\"\"\n",
    "        for d2 in soup2.findAll('div',attrs={'id':'feature-bullets', 'class':'a-section a-spacing-medium a-spacing-top-small'}):\n",
    "            s=\"\"\n",
    "            for d3 in d2.findAll('span',attrs={'class':'a-list-item'}):\n",
    "                s+=(str(d3).replace('</span>','').replace('<span class=\"a-list-item\">','').strip())+\"\\n\"\n",
    "            all1.append(s.splitlines())\n",
    "\n",
    "        \n",
    "        colour=soup2.find('span',attrs={'class':'selection'})\n",
    "        if colour is not None:\n",
    "            all1.append(colour.text.replace('\\n',''))\n",
    "        else:\n",
    "            all1.append(\"\")\n",
    "            \n",
    "\n",
    "#         add all columns to table\n",
    "        alls.append(all1)\n",
    "    return alls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------WEB SCRAPING FROM AMAZON (FUNCTION CALL)----------------------------------------\n",
    "# -----------------------------------------------(Don't run unless urgent)--------------------------------------------------\n",
    "# import time\n",
    "# start_time = time.time()\n",
    "results = []\n",
    "for i in range(4, no_pages+1):\n",
    "    results.append(get_data(i))\n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "df = pd.DataFrame(flatten(results),columns=['Brand Name','Shirt Name','Rating','Price','link','No. of ratings','Features','Colour'])\n",
    "df.to_csv('amazon_products.csv', index=False, encoding='utf-8')\n",
    "# df = pd.read_csv(r\"C:\\Users\\Purva\\Desktop\\Jupyter Notebook\\amazonmix.csv\")\n",
    "# print(\" time taken - %s seconds\"%(time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
